{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipenv install np pandas maxentropy sklearn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from maxentropy import FeatureTransformer, MinDivergenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats.contingency import expected_freq\n",
    "from scipy.special import xlogy\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle of minimum discrimination information\n",
    "> Given an intial model $\\textbf{p}_0$ of a system and a set of constraints derived from empirical evidence, update to the model $\\textbf{p}$ which minimizes the relative entropy $D(\\textbf{p} || \\textbf{p}_0)$\n",
    "\n",
    "Often the initial model is just the uniform distribution, in which case the right intuition for this is \"choose the flattest possible model which meets your constraints\".\n",
    "(Relative entropy is a sort of distance measure between probability distributions, so if $\\textbf{p}_0$ is uniform then it provides a measure of \"flatness\".)\n",
    "\n",
    "This is a standard tool for choosing priors before fitting a model to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: loaded dice\n",
    "After rolling a six sided die (with faces 1, 2, 3, 4, 5, 6) many times you observe that the average of all of the rolls is 4.5.\n",
    "What statistical model should you use to predict the next roll of the die?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the package `maxentropy`, which implements algorithms for maximizing entropy / minimizing relative entropy within the sklearn API.\n",
    "This implementation requires:\n",
    "\n",
    "- A sample space: in this case, just the faces 1, 2, 3, 4, 5, 6\n",
    "- A log prior: $[\\log(\\frac{1}{6}), \\ldots, \\log(\\frac{1}{6})]$ for the uniform prior\n",
    "- A set of features $f_i$: arbitrary functions defined on the sample space\n",
    "- A set of constraints: expressed as $\\mathbb{E}(f_i(X)) = C$ where $C$ is constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample space\n",
    "sample_space = np.linspace(1, 6, 6, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log prior\n",
    "prior = [1/6] * 6\n",
    "log_prior = np.log(prior)\n",
    "\n",
    "@np.vectorize\n",
    "def prior_log_pdf(x):\n",
    "    return log_prior[x-1]  # zero-based indexing offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our constraint is just $\\mathbb{E}(X) = 4.5$, so we only need one feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "features = [identity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one feature, so the constraint is just a single value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint\n",
    "C = [4.5]\n",
    "constraint = np.atleast_2d(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the minimum (relative) entropy distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MinDivergenceModel(features, sample_space, prior_log_pdf=prior_log_pdf)\n",
    "model.fit(constraint)\n",
    "model.probdist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model.probdist(), index=[1,2,3,4,5,6], columns=['probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(model.probdist(), x=df.index, y=df['probability'], labels={'x': 'face', 'y': 'probability'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "1. Repeat the calculation above, using a non-uniform prior of your choice.  For instance, you could set the probability of the face 6 to 0, or make the face 1 overwhelmingly more likely than the others.\n",
    "\n",
    "2. Repeat the calculation above using the following constraints: the average value of the face is 4.5, and the face 2 occurs 20% of the time.  (Hint: to represent the second constraint, use the boolean feature $f(x) = lambda x: x == 2$.)\n",
    "\n",
    "3. Find the best statistical model of a pair of dice, where the average value of the sum is 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
