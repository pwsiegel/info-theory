{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats.contingency import expected_freq\n",
    "from scipy.special import xlogy\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('gdrive/My Drive/Colab Notebooks/reddit_math_ds_train.parquet').reset_index(drop=True)\n",
    "test = pd.read_parquet('gdrive/My Drive/Colab Notebooks/reddit_math_ds_test.parquet').reset_index(drop=True)\n",
    "# train = pd.read_parquet('/Users/paul/data/reddit/reddit_math_ds_train.parquet').reset_index(drop=True)\n",
    "# test = pd.read_parquet('/Users/paul/data/reddit/reddit_math_ds_test.parquet').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', max_features=250)\n",
    "count_model = cv.fit(train['text'])\n",
    "train_counts = pd.DataFrame(cv.transform(train['text']).todense(), columns=cv.get_feature_names())\n",
    "test_counts = pd.DataFrame(cv.transform(test['text']).todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used information theory to measure the relevance of an input feature to a target variable.\n",
    "With a little nudge we can use this to create a simple but powerful machine learning classifier called a _decision tree_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup\n",
    "The starting point is to view a single feature as a simple classifier.\n",
    "In the dataset above, we can use the presence or absence of a given term to try to predict the subreddit label.\n",
    "To train the classifier, we just have to decide which label we should apply if the term is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_counts['number'] != 0, train['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's treat the term `number` as a classifier by predicting that any reddit post containing `number` is in the `math` subreddit and all others are in `datascience`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(\n",
    "    y_true=test['subreddit'] == 'math',\n",
    "    y_pred=test_counts['number'] > 0,\n",
    "    output_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this computation with all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(test_counts.columns, columns=['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['accuracy'] = feature_df['feature'].apply(\n",
    "    lambda feature: classification_report(\n",
    "        y_true=test['subreddit'] == 'math',\n",
    "        y_pred=test_counts[feature] > 0,\n",
    "        output_dict=True\n",
    "    )['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.sort_values(by='accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "For each feature above, use the training data to compute the mutual information of the feature against the class label.\n",
    "Plot mutual information against classifier accuracy, as above.\n",
    "The code for computing mutual information based on a joint density table is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(joint_density: pd.DataFrame) -> float:\n",
    "    '''\n",
    "    Input: DataFrame representing the joint density of a pair of random variables\n",
    "    Output: Mutual information of the two random variables\n",
    "    '''\n",
    "    independent_density = pd.DataFrame(\n",
    "        expected_freq(joint_density),\n",
    "        columns=joint_density.columns,\n",
    "        index=joint_density.index\n",
    "    )\n",
    "    return -(joint_density * np.log2(independent_density / joint_density)).sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers above have no hope of being very accurate because no one term appears in most of the posts.\n",
    "To build a better classifier we'll need to ensemble them, so that the classifier considers more evidence.\n",
    "\n",
    "A decision tree does this by assembling features into a chain of if/then statements; for instance:\n",
    "\n",
    "- If \"number\" is present, output \"math\"\n",
    "- If \"number\" is not present:\n",
    "    - If \"algebra\" is present, output \"math\"\n",
    "    - If \"algebra\" is not present, output \"datascience\"\n",
    "\n",
    "Here we split the \"number is not present\" condition on the \"engineering\" feature; this allows our model to tell a story like \"If the post doesn't contain the term 'number' and it does contain 'engineering' then it's probably in the datascience subreddit\".\n",
    "\n",
    "We could of course keep splitting, adding in more and more features as we go.\n",
    "We can also split on the \"number is present\" branch of the tree, though that branch doesn't have much data so we'll get diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an implementation of the decision tree above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (test_counts['number'] > 0) | ((test_counts['number'] == 0) & (test_counts['algebra'] > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(\n",
    "    y_true=test['subreddit'] == 'math',\n",
    "    y_pred=pred,\n",
    "    output_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Repeat the computation above for all possible features in place of \"algebra\".  Which tree performs best?\n",
    "2. Restrict the training data to include only posts not containing \"number\", and compute the mutual information between every feature and the class labels (\"math\" and \"datascience\").  How do these numbers compare with your accuracy numbers above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
