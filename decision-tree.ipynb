{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.special import xlogy\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('gdrive/My Drive/Colab Notebooks/reddit_calculus.parquet')\n",
    "train = pd.read_parquet('/Users/paul/data/reddit/reddit_math_ds_train.parquet').reset_index(drop=True)\n",
    "test = pd.read_parquet('/Users/paul/data/reddit/reddit_math_ds_test.parquet').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', max_features=250)\n",
    "count_model = cv.fit(train['text'])\n",
    "train_counts = pd.DataFrame(cv.transform(train['text']).todense(), columns=cv.get_feature_names())\n",
    "test_counts = pd.DataFrame(cv.transform(test['text']).todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used information theory to measure the relevance of an input feature to a target variable.\n",
    "With a little nudge we can use this to create a simple but powerful machine learning classifier called a _decision tree_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup\n",
    "The starting point is to view a single feature as a simple classifier.\n",
    "In the dataset above, we can use the presence or absence of a given term to try to predict the subreddit label\n",
    "To train the classifier, we just have to decide which label we should apply if the term is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_counts['engineering'] != 0, train['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_counts['number'] != 0, train['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Pick a term, and decide whether you would predict the subreddit \"math\" or \"datascience\" if that term is present.\n",
    "2. Use the test set to measure the accuracy of this classifier.\n",
    "3. Compute the mutual information of the term you chose and the subreddit label.\n",
    "4. Repeat the above for lots of terms.  What is the relationship between mutual information and classifier accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers above have no hope of being very accurate because no one term appears in most of the posts.\n",
    "To build a better classifier we'll need to ensemble them, so that the classifier considers more evidence.\n",
    "\n",
    "A decision tree does this by assembling features into a chain of if/then statements; for instance:\n",
    "\n",
    "- If \"number\" is present, output \"math\"\n",
    "- If \"number\" is not present:\n",
    "    - If \"engineering\" is present, output \"datascience\"\n",
    "    - If \"engineering is not present, output \"math\"\n",
    "\n",
    "Here we split the \"number is not present\" condition on the \"engineering\" feature; this allows our model to tell a story like \"If the post doesn't contain the term 'number' and it does contain 'engineering' then it's probably in the datascience subreddit\".\n",
    "\n",
    "We could of course keep splitting, adding in more and more features as we go.\n",
    "We can also split on the \"number is present\" branch of the tree, though that branch doesn't have much data so we'll get diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_number = train_counts[train_counts['number'] == 0]\n",
    "pd.crosstab(not_number['engineering'] != 0, train['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Compute the accuracy of the classifier described above on the test set.\n",
    "2. Create your own decision tree, and calculate its accuracy.  How high can you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing splits by hand is neither scalable nor particularly principled, but we can use information theory to do better.\n",
    "\n",
    "Idea: given a tree $T$, choose the feature $F$ to split on so that it maximizes information gain:\n",
    "\n",
    "$$I(T,F) = H(T) - H(T \\vert F)$$\n",
    "\n",
    "Next time: how and why do we maximize information gain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
