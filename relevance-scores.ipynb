{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.special import xlogy\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('gdrive/My Drive/Colab Notebooks/reddit_calculus.parquet')\n",
    "# df = pd.read_parquet('/Users/paul/data/reddit/reddit_calculus.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit'] = df['url'].apply(lambda url: url.split('/')[4].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', max_features=250)\n",
    "count_matrix = cv.fit_transform(df['text'])\n",
    "freq = pd.DataFrame(count_matrix.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information theory and relvance scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "Let $D$ represent a randomly chosen document from a corpus, and let $T$ represent a randomly chosen term.\n",
    "Then:\n",
    "\n",
    "$$I(D,T) = H(D) - H(D \\vert T)$$\n",
    "\n",
    "(\"Mutual information is entropy minus conditional entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "- $N$ = number of documents\n",
    "- $N_t$ = number of documents containing the term $t$\n",
    "- $\\vert d \\vert$ = number of terms in document $d$\n",
    "- $f(t,d)$ = frequency of term $t$ in document $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutual information is:\n",
    "\n",
    "$$I(D,T) = \\sum_{d,t} \\frac{f(t,d)}{\\vert d \\vert} \\left( \\log \\frac{1}{N_t} - \\log \\frac{1}{N} \\right) = \\sum_{d,t} \\frac{f(t,d)}{\\vert d \\vert} \\log \\frac{N}{N_t}$$\n",
    "\n",
    "The pointwise mutual information is then:\n",
    "\n",
    "$$\\frac{f(t,d)}{\\vert d \\vert} \\left( \\log \\frac{1}{N_t} - \\log \\frac{1}{N} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "- All documents have equal weight\n",
    "- All documents containing a given term have equal weight\n",
    "- All terms in a given document are equally likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = freq.sum(axis=1)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_freq = freq.div(d, axis=0)\n",
    "normalized_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_t = (normalized_freq != 0).sum(axis=0)\n",
    "N_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(freq)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI = normalized_freq * (np.log2(1/N_t) - np.log2(1/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "1. Write a function that views documents and the term relevance scores side-by-side.\n",
    "2. Pick a few subreddits to declare as \"irrelevant\" and compute the document PMI matrix that you would get if the posts from those subreddits had one tenth of their normal weight. How does this affect the document-level relevance scores?\n",
    "3. Compute the PMI matrix for the top 20 subreddits, treating all posts in the subreddit as a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
